# Data Importing

Following the [schema design](2.%20Schema%20Design.md), we turn our attention to data cleaning, converting data to a format compatible for importing into Dgraph, and finally importing the data into a Dgraph cluster.

## Data Cleaning/Prepping

As mentioned in the [earlier note](2. Schema Design.md), we processed the raw Offshore Leak CSV files and stored the data in Badger[^1], an open source key-value store maintained by Dgraph.

## Conversion of Data to Dgraph-importable Format

We expand the already created corresponding models in the [/tools/model](/tools/model) folder to export RDF representations of the data. Take for instance the RDF encoding function for the Entity type:

```golang
func (e *Entity) ToRDF(w io.Writer) {
	e.Normalize()
	id := e.RDFID()
	fmt.Fprintf(w, "%s <dgraph.type> \"Entity\" .\n", id)
	e.Record.ToRDF(w)
	RDFEncodeTriples(w, tstore.TriplesFromStruct(id, e))
}
```

This function first _normalizes_ (`e.Normalize()`) the record by looking for terms that are "boilerplate", such as 'None' for the entity name, or converting jurisdiction codes of XX (a data entry mistake) to XXX (XXX = "Unknown Jurisdiction"). Ultimately, the function RDF encodes all predicates of the record to the stream passed.

Programs in [/tools/export](/tools/export) and [/tools/export-subset](/tools/export-subset/) load all (or a random subset of) Entities, Others, Officers, Intermediaries and Addresses records and run the `ToRDF()` function that write the results to a configurable output folder (/rdf or /rdf-subset). Finally, these programs create relationships between the records by iterating the [Relationship](/tools/model/relationship.go) type from the Badger store. Tests in the model package, such as the [entity test](/tools/model/entity_test.go), are a good example of what we expect in the conversion from CSV-parsed to Dgraph-compatible RDF-encoded data.

Note, one aspect of converting Address records to RDF data involves consulting a pre-loaded CSV of geo-encoded data. See the section below describing Geo-encoding of Address Records.

Exporting all Records to Dgraph-compatible RDF output:
```bash
cd tools
go run export/main.go
```

Exporting a subset (50,000 random relationships):
```bash
cd tools
go run export-subset/main.go
```

Both programs write Dgraph-compatible RDF files in the `/export `or `/export-subset` (respective) folders. We'll use these RDF files in a subsequent section to populate a Dgraph cluster.

### Geo-encoding of Address Records

One stated goal for the VLG is the ability to query Address records by geo coordinates. The Offshore Leaks dataset address entries are non-standardized, often with formatting errors that make it difficult to get normalized address information from the entries.

We decided to attempt to geo-encode the US-based addresses from the dataset. Of the 24,366 US-based addresses, we were only able to successfully geo-encode 17,893 of those (~73%).

In [/geo/main.go](/tools/geo/main.go), we use two libraries, libpostal[^2], a library for parsing/normalizing street addresses using statistical NLP and open geo data, and the US Census Bureau's Geodecoder API[^3] to convert addresses to geo-coordinates.

First, we launch a convenient Docker container for libpostal

```bash
docker run -d -p 8899:8080 clicksend/libpostal-rest
```

The geo-encoding [program]((/tools/geo/main.go)) asks libpostal to normalize the raw address line to normalized number, street, city and state sections. If successful, it makes a request to the US Census Bureau Geodecoder API for the latitude and longitude of the address. If and only if that step is successful, it writes the result (lat/long) to a CSV file [/tools/addresses-geoencoded.csv](/tools/addresses-geoencoded.csv)â€”this file will be used during RDF conversion (see the Address.location field in the GraphQL SDL schema).

Note that we could have written the results of the geo-encoding directly into the Badger store for each Address record that we successfully processed. However, the process described in this section takes several hours to resolve, so we chose to write it to a persistent CSV [file](/tools/addresses-geoencoded.csv) in the repo so those following along step-by-step would not be impeded by this lengthy process.

## Importing into Dgraph

At this point, we're ready to start a Dgraph cluster, apply the schema and load the RDF data that we just converted. For small datasets (such as the subset described above), the [Live Loader](https://dgraph.io/docs/howto/importdata/live-loader/) is recommended.

For initial loading of a new graph, or for import sizes larger than a million edges, the [Bulk Loader](https://dgraph.io/docs/howto/importdata/bulk-loader/) is the appropriate mechanism.

We'll describe both procedures below.

### Bulk Loading

### Live Loading a Subset


[^1]: [dgraph/badger](https://github.com/dgraph-io/badger)
[^2]: [libpostal](https://github.com/openvenues/libpostal)
[^3]: [US Census Geo-encoding REST API](https://geocoding.geo.census.gov/geocoder/)

